{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8be82-fca2-473a-996e-8fcea6887e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+------------------+\n",
      "| Model | Accuracy | Precision | Confusion Matrix |\n",
      "+-------+----------+-----------+------------------+\n",
      "|  mnb  |  0.9578  |   1.0000  |    [[957   0]    |\n",
      "|       |          |           |    [ 47 111]]    |\n",
      "|   xg  |  0.9614  |   0.9675  |    [[953   4]    |\n",
      "|       |          |           |    [ 39 119]]    |\n",
      "+-------+----------+-----------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import email\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "import string\n",
    "import pickle\n",
    "from prettytable import PrettyTable  # For displaying results in a table format\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Function to preprocess and clean the email text\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [i for i in text if i.isalnum()]\n",
    "    text = [i for i in text if i not in stopwords.words('english') and i not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(i, pos='v') for i in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "# Metadata extraction function\n",
    "def extract_metadata(email_content):\n",
    "    msg = email.message_from_string(email_content)\n",
    "    sender = msg[\"From\"] or \"\"\n",
    "    domain = sender.split(\"@\")[-1] if \"@\" in sender else \"unknown\"\n",
    "    reply_to = msg[\"Reply-To\"] or \"\"\n",
    "    subject = msg[\"Subject\"] or \"\"\n",
    "\n",
    "    features = {\n",
    "        \"Sender_Domain\": domain if domain != \"unknown\" else \"unknown_domain\",\n",
    "        \"Reply_To_Mismatch\": int(reply_to != \"\" and reply_to != sender),\n",
    "        \"Subject_Length\": len(subject),\n",
    "        \"Is_HTML\": int(msg.get_content_type() == \"text/html\"),\n",
    "        \"Num_Links\": len(re.findall(r\"https?://[^\\s]+\", msg.get_payload(decode=True).decode(errors=\"ignore\") if msg.get_payload() else \"\")),\n",
    "        \"Num_Attachments\": sum(1 for part in msg.walk() if part.get_content_disposition() == \"attachment\"),\n",
    "        \"Is_Shortened_URL\": int(any(tldextract.extract(url).domain in {\"bit.ly\", \"tinyurl.com\"} for url in re.findall(r\"https?://[^\\s]+\", msg.get_payload(decode=True).decode(errors=\"ignore\") if msg.get_payload() else \"\")))\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "# Combine text and metadata features\n",
    "def process_email(email_content, email_text):\n",
    "    metadata = extract_metadata(email_content)\n",
    "    transformed_text = transform_text(email_text)\n",
    "    metadata[\"Processed_Text\"] = transformed_text\n",
    "    return metadata\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859â€“1')\n",
    "df.rename(columns={'v1':'target', 'v2':'text'}, inplace=True)\n",
    "\n",
    "# Apply text transformation and metadata extraction\n",
    "df['metadata_features'] = df.apply(lambda row: process_email(row['text'], row['text']), axis=1)\n",
    "\n",
    "# Convert metadata features into separate columns\n",
    "metadata_df = pd.json_normalize(df['metadata_features'])\n",
    "df = pd.concat([df, metadata_df], axis=1)\n",
    "df.drop(columns=['metadata_features'], inplace=True)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['target'])\n",
    "\n",
    "# Encode categorical features (Sender_Domain column)\n",
    "domain_encoder = LabelEncoder()\n",
    "df['Sender_Domain'] = domain_encoder.fit_transform(df['Sender_Domain'])\n",
    "\n",
    "# Feature extraction (TF-IDF for text)\n",
    "tf = TfidfVectorizer()\n",
    "X_text = tf.fit_transform(df['Processed_Text']).toarray()\n",
    "\n",
    "# Combine metadata features with text features\n",
    "metadata_columns = ['Sender_Domain', 'Reply_To_Mismatch', 'Subject_Length', 'Is_HTML', 'Num_Links', 'Num_Attachments', 'Is_Shortened_URL']\n",
    "X_metadata = df[metadata_columns].values\n",
    "X = np.hstack([X_text, X_metadata])\n",
    "\n",
    "y = df['target'].values\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Models to evaluate\n",
    "mls = { \n",
    "    'mnb': MultinomialNB(),\n",
    "    'xg': XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Training and evaluation\n",
    "results = []\n",
    "\n",
    "# Create a PrettyTable object for formatted output\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"Accuracy\", \"Precision\", \"Confusion Matrix\"]\n",
    "\n",
    "for name, model in mls.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ps = precision_score(y_test, y_pred)\n",
    "        results.append([name, acc, ps, cm.tolist()])\n",
    "\n",
    "        # Add the result to the table\n",
    "        table.add_row([name, f\"{acc:.4f}\", f\"{ps:.4f}\", str(cm)])\n",
    "    except Exception as e:\n",
    "        results.append([name, 'Error', 'Error', str(e)])\n",
    "\n",
    "\n",
    "print(table)\n",
    "\n",
    "# Save model and vectorizer\n",
    "pickle.dump(tf, open('vectorizer.pkl', 'wb'))\n",
    "pickle.dump(mls['mnb'], open('model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655f458-d9a0-4d24-ab29-cda318eaa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce95dc5-8e6e-4fef-b872-8dadc6550846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
